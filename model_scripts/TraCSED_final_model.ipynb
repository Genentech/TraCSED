{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8330db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 11:21:17.443586: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-16 11:21:22.881461: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-01-16 11:21:22.886209: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-16 11:21:33.278414: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Flatten\n",
    "from keras import backend as K\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "import os\n",
    "os.chdir('/gstore/project/hr_brca_heterogeneity/T47D_trace_Seq_v2/Nathan')\n",
    "#os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c1b436",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Hyperparameters'''\n",
    "\n",
    "batch_size = 32 # was 32\n",
    "seq_len = 8 # was 128\n",
    "\n",
    "#d_k = 256\n",
    "#d_v = 256\n",
    "n_heads = 40 # was 60  # Number of attention heads # no difference or imporvement between 5-60\n",
    "n_head_size= 5 # was 46 # Embedding size for attention # no difference between 5-46\n",
    "n_ff_dim = 5  # was 55    # Hidden layer size in feed forward network inside transformer # some difference between 5-55\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cc8b191",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### full data\n",
    "import json\n",
    "\n",
    "# tran_9545_plsrres0.3_HALL3.json\n",
    "# tran_palbo_plsrres0.3_HALL3.json\n",
    "# tran_combo_plsrres0.5_HALL3.json\n",
    "f = open('json_files/tran_9545_plsrres0.3_HALL3.json')\n",
    "data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2324e80a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pseudotime</th>\n",
       "      <th>RealTime</th>\n",
       "      <th>Survival</th>\n",
       "      <th>Score.MAPK</th>\n",
       "      <th>Score.ER_induced</th>\n",
       "      <th>Score.ER_repressed</th>\n",
       "      <th>Score.Hippo_145</th>\n",
       "      <th>Score.Hippo_50</th>\n",
       "      <th>Score.TCGA_LuBrCa_mPIK3CA_up</th>\n",
       "      <th>Score.TCGA_LuBrCa_mPIK3CA_dn</th>\n",
       "      <th>...</th>\n",
       "      <th>Score.HALLMARK_ANGIOGENESIS</th>\n",
       "      <th>Score.HALLMARK_HEME_METABOLISM</th>\n",
       "      <th>Score.HALLMARK_COAGULATION</th>\n",
       "      <th>Score.HALLMARK_IL2_STAT5_SIGNALING</th>\n",
       "      <th>Score.HALLMARK_BILE_ACID_METABOLISM</th>\n",
       "      <th>Score.HALLMARK_PEROXISOME</th>\n",
       "      <th>Score.HALLMARK_ALLOGRAFT_REJECTION</th>\n",
       "      <th>Score.HALLMARK_SPERMATOGENESIS</th>\n",
       "      <th>Score.HALLMARK_KRAS_SIGNALING_UP</th>\n",
       "      <th>Score.HALLMARK_KRAS_SIGNALING_DN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0098</td>\n",
       "      <td>1</td>\n",
       "      <td>0.3518</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.3627</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.2336</td>\n",
       "      <td>0.1245</td>\n",
       "      <td>0.0844</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8933</td>\n",
       "      <td>1.7672</td>\n",
       "      <td>2.3216</td>\n",
       "      <td>1.6280</td>\n",
       "      <td>1.5111</td>\n",
       "      <td>1.9565</td>\n",
       "      <td>2.2742</td>\n",
       "      <td>1.6578</td>\n",
       "      <td>2.1438</td>\n",
       "      <td>0.6785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0105</td>\n",
       "      <td>2</td>\n",
       "      <td>0.3143</td>\n",
       "      <td>0.1703</td>\n",
       "      <td>0.2965</td>\n",
       "      <td>0.3819</td>\n",
       "      <td>0.4214</td>\n",
       "      <td>0.3118</td>\n",
       "      <td>0.0888</td>\n",
       "      <td>0.1282</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0071</td>\n",
       "      <td>1.7374</td>\n",
       "      <td>1.9475</td>\n",
       "      <td>1.7186</td>\n",
       "      <td>2.0668</td>\n",
       "      <td>2.1993</td>\n",
       "      <td>2.1643</td>\n",
       "      <td>1.4647</td>\n",
       "      <td>1.9337</td>\n",
       "      <td>1.6546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0116</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2891</td>\n",
       "      <td>0.2174</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>0.5001</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.2268</td>\n",
       "      <td>0.0770</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.6901</td>\n",
       "      <td>1.2966</td>\n",
       "      <td>2.1445</td>\n",
       "      <td>1.5817</td>\n",
       "      <td>1.9198</td>\n",
       "      <td>2.1749</td>\n",
       "      <td>1.7421</td>\n",
       "      <td>2.0901</td>\n",
       "      <td>1.9001</td>\n",
       "      <td>1.4106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0129</td>\n",
       "      <td>1</td>\n",
       "      <td>0.2772</td>\n",
       "      <td>0.4634</td>\n",
       "      <td>0.5483</td>\n",
       "      <td>0.2941</td>\n",
       "      <td>0.3515</td>\n",
       "      <td>0.2339</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>0.3133</td>\n",
       "      <td>...</td>\n",
       "      <td>2.1376</td>\n",
       "      <td>1.7490</td>\n",
       "      <td>2.1339</td>\n",
       "      <td>1.6222</td>\n",
       "      <td>1.5716</td>\n",
       "      <td>2.1146</td>\n",
       "      <td>2.1199</td>\n",
       "      <td>1.9977</td>\n",
       "      <td>2.0230</td>\n",
       "      <td>1.5737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0143</td>\n",
       "      <td>2</td>\n",
       "      <td>0.2668</td>\n",
       "      <td>0.4727</td>\n",
       "      <td>0.4183</td>\n",
       "      <td>0.3784</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.3497</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7691</td>\n",
       "      <td>1.4083</td>\n",
       "      <td>2.1572</td>\n",
       "      <td>1.6870</td>\n",
       "      <td>1.8793</td>\n",
       "      <td>2.3432</td>\n",
       "      <td>2.2296</td>\n",
       "      <td>1.8866</td>\n",
       "      <td>1.8935</td>\n",
       "      <td>1.3916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>0.9984</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0899</td>\n",
       "      <td>0.1211</td>\n",
       "      <td>0.2532</td>\n",
       "      <td>0.4466</td>\n",
       "      <td>0.4576</td>\n",
       "      <td>0.5640</td>\n",
       "      <td>0.3350</td>\n",
       "      <td>0.1142</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0554</td>\n",
       "      <td>1.5934</td>\n",
       "      <td>1.6054</td>\n",
       "      <td>1.7091</td>\n",
       "      <td>1.5929</td>\n",
       "      <td>1.5802</td>\n",
       "      <td>1.8458</td>\n",
       "      <td>1.5784</td>\n",
       "      <td>1.9996</td>\n",
       "      <td>1.5064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2381</th>\n",
       "      <td>0.9991</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0928</td>\n",
       "      <td>0.1573</td>\n",
       "      <td>0.3216</td>\n",
       "      <td>0.6288</td>\n",
       "      <td>0.5804</td>\n",
       "      <td>0.6759</td>\n",
       "      <td>0.2311</td>\n",
       "      <td>0.0567</td>\n",
       "      <td>...</td>\n",
       "      <td>1.5983</td>\n",
       "      <td>1.6818</td>\n",
       "      <td>1.7428</td>\n",
       "      <td>1.6946</td>\n",
       "      <td>1.5457</td>\n",
       "      <td>1.3229</td>\n",
       "      <td>1.9067</td>\n",
       "      <td>1.4309</td>\n",
       "      <td>1.5273</td>\n",
       "      <td>1.7536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2382</th>\n",
       "      <td>0.9995</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0841</td>\n",
       "      <td>0.1324</td>\n",
       "      <td>0.4018</td>\n",
       "      <td>0.7432</td>\n",
       "      <td>0.4769</td>\n",
       "      <td>0.5029</td>\n",
       "      <td>0.2264</td>\n",
       "      <td>0.0948</td>\n",
       "      <td>...</td>\n",
       "      <td>1.1106</td>\n",
       "      <td>1.4459</td>\n",
       "      <td>1.3601</td>\n",
       "      <td>1.7555</td>\n",
       "      <td>1.7549</td>\n",
       "      <td>1.4490</td>\n",
       "      <td>1.7661</td>\n",
       "      <td>1.5650</td>\n",
       "      <td>1.8006</td>\n",
       "      <td>1.8729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2383</th>\n",
       "      <td>0.9999</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0745</td>\n",
       "      <td>0.1656</td>\n",
       "      <td>0.2699</td>\n",
       "      <td>0.6598</td>\n",
       "      <td>0.4079</td>\n",
       "      <td>0.5167</td>\n",
       "      <td>0.3009</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>...</td>\n",
       "      <td>1.7250</td>\n",
       "      <td>1.6574</td>\n",
       "      <td>1.7760</td>\n",
       "      <td>2.1085</td>\n",
       "      <td>1.8774</td>\n",
       "      <td>1.5012</td>\n",
       "      <td>1.6859</td>\n",
       "      <td>1.4455</td>\n",
       "      <td>1.6761</td>\n",
       "      <td>1.6979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>1.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1307</td>\n",
       "      <td>0.3544</td>\n",
       "      <td>0.4496</td>\n",
       "      <td>0.4181</td>\n",
       "      <td>0.4188</td>\n",
       "      <td>0.1479</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>...</td>\n",
       "      <td>2.3420</td>\n",
       "      <td>1.8068</td>\n",
       "      <td>1.5101</td>\n",
       "      <td>1.8005</td>\n",
       "      <td>1.8182</td>\n",
       "      <td>1.4481</td>\n",
       "      <td>1.5664</td>\n",
       "      <td>1.7731</td>\n",
       "      <td>1.8109</td>\n",
       "      <td>1.6155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2385 rows × 75 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Pseudotime  RealTime  Survival  Score.MAPK  Score.ER_induced  \\\n",
       "0         0.0098         1    0.3518      0.0000            0.3627   \n",
       "1         0.0105         2    0.3143      0.1703            0.2965   \n",
       "2         0.0116         1    0.2891      0.2174            0.5001   \n",
       "3         0.0129         1    0.2772      0.4634            0.5483   \n",
       "4         0.0143         2    0.2668      0.4727            0.4183   \n",
       "...          ...       ...       ...         ...               ...   \n",
       "2380      0.9984         5    0.0899      0.1211            0.2532   \n",
       "2381      0.9991         5    0.0928      0.1573            0.3216   \n",
       "2382      0.9995         5    0.0841      0.1324            0.4018   \n",
       "2383      0.9999         5    0.0745      0.1656            0.2699   \n",
       "2384      1.0000         3    0.0598      0.1307            0.3544   \n",
       "\n",
       "      Score.ER_repressed  Score.Hippo_145  Score.Hippo_50  \\\n",
       "0                 0.4066           0.2999          0.2336   \n",
       "1                 0.3819           0.4214          0.3118   \n",
       "2                 0.5001           0.3403          0.2268   \n",
       "3                 0.2941           0.3515          0.2339   \n",
       "4                 0.3784           0.4032          0.3497   \n",
       "...                  ...              ...             ...   \n",
       "2380              0.4466           0.4576          0.5640   \n",
       "2381              0.6288           0.5804          0.6759   \n",
       "2382              0.7432           0.4769          0.5029   \n",
       "2383              0.6598           0.4079          0.5167   \n",
       "2384              0.4496           0.4181          0.4188   \n",
       "\n",
       "      Score.TCGA_LuBrCa_mPIK3CA_up  Score.TCGA_LuBrCa_mPIK3CA_dn  ...  \\\n",
       "0                           0.1245                        0.0844  ...   \n",
       "1                           0.0888                        0.1282  ...   \n",
       "2                           0.0770                        0.0000  ...   \n",
       "3                           0.1068                        0.3133  ...   \n",
       "4                           0.1019                        0.2250  ...   \n",
       "...                            ...                           ...  ...   \n",
       "2380                        0.3350                        0.1142  ...   \n",
       "2381                        0.2311                        0.0567  ...   \n",
       "2382                        0.2264                        0.0948  ...   \n",
       "2383                        0.3009                        0.1464  ...   \n",
       "2384                        0.1479                        0.1875  ...   \n",
       "\n",
       "      Score.HALLMARK_ANGIOGENESIS  Score.HALLMARK_HEME_METABOLISM  \\\n",
       "0                          0.8933                          1.7672   \n",
       "1                          2.0071                          1.7374   \n",
       "2                          1.6901                          1.2966   \n",
       "3                          2.1376                          1.7490   \n",
       "4                          1.7691                          1.4083   \n",
       "...                           ...                             ...   \n",
       "2380                       2.0554                          1.5934   \n",
       "2381                       1.5983                          1.6818   \n",
       "2382                       1.1106                          1.4459   \n",
       "2383                       1.7250                          1.6574   \n",
       "2384                       2.3420                          1.8068   \n",
       "\n",
       "      Score.HALLMARK_COAGULATION  Score.HALLMARK_IL2_STAT5_SIGNALING  \\\n",
       "0                         2.3216                              1.6280   \n",
       "1                         1.9475                              1.7186   \n",
       "2                         2.1445                              1.5817   \n",
       "3                         2.1339                              1.6222   \n",
       "4                         2.1572                              1.6870   \n",
       "...                          ...                                 ...   \n",
       "2380                      1.6054                              1.7091   \n",
       "2381                      1.7428                              1.6946   \n",
       "2382                      1.3601                              1.7555   \n",
       "2383                      1.7760                              2.1085   \n",
       "2384                      1.5101                              1.8005   \n",
       "\n",
       "      Score.HALLMARK_BILE_ACID_METABOLISM  Score.HALLMARK_PEROXISOME  \\\n",
       "0                                  1.5111                     1.9565   \n",
       "1                                  2.0668                     2.1993   \n",
       "2                                  1.9198                     2.1749   \n",
       "3                                  1.5716                     2.1146   \n",
       "4                                  1.8793                     2.3432   \n",
       "...                                   ...                        ...   \n",
       "2380                               1.5929                     1.5802   \n",
       "2381                               1.5457                     1.3229   \n",
       "2382                               1.7549                     1.4490   \n",
       "2383                               1.8774                     1.5012   \n",
       "2384                               1.8182                     1.4481   \n",
       "\n",
       "      Score.HALLMARK_ALLOGRAFT_REJECTION  Score.HALLMARK_SPERMATOGENESIS  \\\n",
       "0                                 2.2742                          1.6578   \n",
       "1                                 2.1643                          1.4647   \n",
       "2                                 1.7421                          2.0901   \n",
       "3                                 2.1199                          1.9977   \n",
       "4                                 2.2296                          1.8866   \n",
       "...                                  ...                             ...   \n",
       "2380                              1.8458                          1.5784   \n",
       "2381                              1.9067                          1.4309   \n",
       "2382                              1.7661                          1.5650   \n",
       "2383                              1.6859                          1.4455   \n",
       "2384                              1.5664                          1.7731   \n",
       "\n",
       "      Score.HALLMARK_KRAS_SIGNALING_UP  Score.HALLMARK_KRAS_SIGNALING_DN  \n",
       "0                               2.1438                            0.6785  \n",
       "1                               1.9337                            1.6546  \n",
       "2                               1.9001                            1.4106  \n",
       "3                               2.0230                            1.5737  \n",
       "4                               1.8935                            1.3916  \n",
       "...                                ...                               ...  \n",
       "2380                            1.9996                            1.5064  \n",
       "2381                            1.5273                            1.7536  \n",
       "2382                            1.8006                            1.8729  \n",
       "2383                            1.6761                            1.6979  \n",
       "2384                            1.8109                            1.6155  \n",
       "\n",
       "[2385 rows x 75 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_days = {'Day 0': 1, 'Day 1': 2, 'Day 4': 3, 'Day 8': 4, 'Day 26': 5}\n",
    "\n",
    "df = pd.DataFrame(data['GFPBC_libB_90850'])\n",
    "df.iloc[:, 0:len(df.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e872d503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['GFPBC_libB_92196', 'GFPBC_libB_20182', 'GFPBC_libB_90850', 'GFPBC_libB_10678', 'GFPBC_libB_37798', 'GFPBC_libB_31581', 'GFPBC_libB_40758', 'GFPBC_libB_94397', 'GFPBC_libB_83026', 'GFPBC_libB_10737']\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "barcodes = [i for i in data.keys()]\n",
    "new_barcodes = []\n",
    "for i in barcodes:\n",
    "    df = pd.DataFrame(data[i])\n",
    "    #print(i, df.shape[0])\n",
    "    if df.shape[0] > 200: # 50 or 60\n",
    "        new_barcodes.append(i)\n",
    "print(new_barcodes)\n",
    "print(len(new_barcodes))\n",
    "\n",
    "bar_dict = {new_barcodes[i]:i for i in range(len(new_barcodes))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e5a6d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create training and test data'''\n",
    "\n",
    "def transformer_train_test_split_dayiter(bar, frac_val, extra_var_bool, splitval):\n",
    "    \n",
    "    bardf = pd.DataFrame(data[bar])\n",
    "    #bardf = bardf[bardf['RealTime'] <= exp_days[d]]\n",
    "    num_cell_day = len(bardf['RealTime'].tolist())\n",
    "    snum = int(num_cell_day*splitval)\n",
    "    bardf = bardf.iloc[:snum,:]\n",
    "    \n",
    "    \n",
    "    bardf = bardf.iloc[:,:len(bardf.columns)-1]\n",
    "    bardf['RealTime'] = bardf['RealTime'].astype('category')\n",
    "    bardf['Barcode'] = [bar_dict[bar] for _ in range(bardf.shape[0])]\n",
    "    bardf['Barcode'] = bardf['Barcode'].astype('category')\n",
    "    \n",
    "    # normalize after train/test split\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    bardf2 = pd.DataFrame(scaler.fit_transform(bardf.iloc[:,2:len(bardf.columns)-1])) # survival and expression\n",
    "    bardf2.columns = bardf.columns[2:len(bardf.columns)-1]\n",
    "    \n",
    "    if extra_var_bool[0]:\n",
    "        bardf2['RealTime'] = bardf['RealTime'].astype('category')\n",
    "        num_cell_day = len(bardf2['RealTime'].tolist())\n",
    "    if extra_var_bool[1]:\n",
    "        bardf2['Barcode'] = bardf['Barcode'].astype('category')\n",
    "    if extra_var_bool[2]:\n",
    "        bardf2['Pseudotime'] = bardf['Pseudotime']\n",
    "    #bardf2 = bardf2.dropna()\n",
    "    COLS = bardf2.columns.tolist()[1:]\n",
    "    #print(COLS)\n",
    "    \n",
    "    bardf2 = bardf2.dropna()\n",
    "    bardf2.index = [i for i in range(bardf2.shape[0])]\n",
    "    bardf_train = bardf2[:-(round(num_cell_day*frac_val))]\n",
    "    bardf_val = bardf2[len(bardf2) - (round(num_cell_day*frac_val)):]\n",
    "    #print(bardf_val.shape, bardf_train.shape)\n",
    "    \n",
    "    train_y = bardf_train['Survival']\n",
    "    train_data = bardf_train.iloc[:,1:]\n",
    "    val_y = bardf_val['Survival'].tolist()\n",
    "    val_data = bardf_val.iloc[:,1:]\n",
    "    \n",
    "    if frac_val == 0:\n",
    "        train_data = bardf2.iloc[:,1:]\n",
    "        train_y = bardf2['Survival']\n",
    "    \n",
    "    # Training data\n",
    "    X_train, y_train = [], []\n",
    "    for i in range(seq_len, len(train_data)):\n",
    "      X_train.append(train_data[i-seq_len:i]) \n",
    "      y_train.append(train_y[i]) #Value of 0th column (Survival) of df-row seq_len+1\n",
    "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
    "    \n",
    "    if frac_val != 0:\n",
    "    \n",
    "        # Validation data\n",
    "        X_val, y_val = [], []\n",
    "        for i in range(seq_len, len(val_data)):\n",
    "          X_val.append(val_data[i-seq_len:i]) \n",
    "          y_val.append(val_y[i]) #Value of 0th column (Survival) of df-row seq_len+1\n",
    "        X_val, y_val = np.array(X_val), np.array(y_val)\n",
    "    \n",
    "        print('Transformer Training/Val set shapes (varies depending on barcode) ', X_train.shape, X_val.shape)\n",
    "        return X_train, y_train, X_val, y_val, COLS\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        print('Transformer Training/Val set shapes (varies depending on barcode) ', X_train.shape)\n",
    "        return X_train, y_train, COLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79342f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create training and test data'''\n",
    "\n",
    "def regr_train_test_split_dayiter(bar, frac_val, extra_var_bool, splitval):\n",
    "    \n",
    "    bardf = pd.DataFrame(data[bar])\n",
    "    #bardf = bardf[bardf['RealTime'] <= exp_days[d]]\n",
    "    num_cell_day = len(bardf['RealTime'].tolist())\n",
    "    snum = int(num_cell_day*splitval)\n",
    "    bardf = bardf.iloc[:snum,:]\n",
    "    \n",
    "    bardf = bardf.iloc[:,:len(bardf.columns)-1]\n",
    "    bardf['RealTime'] = bardf['RealTime'].astype('category')\n",
    "    bardf['Barcode'] = [bar_dict[bar] for _ in range(bardf.shape[0])]\n",
    "    bardf['Barcode'] = bardf['Barcode'].astype('category')\n",
    "    \n",
    "    # normalize after train/test split\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1)) \n",
    "    bardf2 = pd.DataFrame(scaler.fit_transform(bardf.iloc[:,2:len(bardf.columns)-1])) # survival and expression\n",
    "    bardf2.columns = bardf.columns[2:len(bardf.columns)-1]\n",
    "    \n",
    "    if extra_var_bool[0]:\n",
    "        bardf2['RealTime'] = bardf['RealTime'].astype('category')\n",
    "        num_cell_day = len(bardf2['RealTime'].tolist())\n",
    "    if extra_var_bool[1]:\n",
    "        bardf2['Barcode'] = bardf['Barcode'].astype('category')\n",
    "    if extra_var_bool[2]:\n",
    "        bardf2['Pseudotime'] = bardf['Pseudotime']\n",
    "        \n",
    "    bardf2 = bardf2.dropna()\n",
    "    bardf2.index = [i for i in range(bardf2.shape[0])]\n",
    "    bardf_train = bardf2[:-(round(num_cell_day*frac_val))]\n",
    "    bardf_val = bardf2[len(bardf2) - (round(num_cell_day*frac_val)):]\n",
    "    #print(bardf_val)\n",
    "    \n",
    "    train_y = bardf_train['Survival']\n",
    "    train_data = bardf_train.iloc[:,1:]\n",
    "    val_y = bardf_val['Survival'].tolist()\n",
    "    val_data = bardf_val.iloc[:,1:]\n",
    "    \n",
    "    # Training data\n",
    "    X_train, y_train = np.array(train_data), np.array(train_y)\n",
    "    \n",
    "    # Validation data\n",
    "    X_val, y_val = np.array(val_data), np.array(val_y)\n",
    "    \n",
    "    print('Regr Training/Val set shapes (varies depending on barcode) ', X_train.shape, X_val.shape)\n",
    "    return X_train, y_train, X_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30c6d36c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer Training/Val set shapes (varies depending on barcode)  (690, 8, 72) (167, 8, 72)\n",
      "Regr Training/Val set shapes (varies depending on barcode)  (698, 72) (175, 72)\n",
      "Transformer Training/Val set shapes (varies depending on barcode)  (1564, 8, 72) (167, 8, 72)\n",
      "Regr Training/Val set shapes (varies depending on barcode)  (1572, 72) (175, 72)\n",
      "Transformer Training/Val set shapes (varies depending on barcode)  (2438, 8, 72) (167, 8, 72)\n",
      "Regr Training/Val set shapes (varies depending on barcode)  (2446, 72) (175, 72)\n",
      "Transformer Training/Val set shapes (varies depending on barcode)  (3312, 8, 72) (167, 8, 72)\n",
      "Regr Training/Val set shapes (varies depending on barcode)  (3320, 72) (175, 72)\n",
      "Transformer Training/Val set shapes (varies depending on barcode)  (3487, 8, 72)\n"
     ]
    }
   ],
   "source": [
    "########### testing function #################\n",
    "# make the training and validation data\n",
    "\n",
    "b = 'GFPBC_libB_92196'\n",
    "frac_val = 0.20\n",
    "extra_var_bool = [False, False, False] # 0-realtime, 1-barcode, 2-pseudotime\n",
    "vs = [0.25, 0.5, 0.75, 1]\n",
    "pct_test = frac_val/len(vs)\n",
    "for v in vs:\n",
    "    X_train, y_train, X_val, y_val, cols = transformer_train_test_split_dayiter(b, pct_test, extra_var_bool, v)\n",
    "    X_train, y_train, X_val, y_val = regr_train_test_split_dayiter(b, pct_test, extra_var_bool, v)\n",
    "\n",
    "X_full, y_full, COLS = transformer_train_test_split_dayiter(b, 0, extra_var_bool, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c6f9e261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, num_attention_blocks, num_transformer_blocks, dropout=0):\n",
    "    if num_transformer_blocks == 0 and num_attention_blocks == 0:\n",
    "        return inputs\n",
    "    \n",
    "    if num_attention_blocks != 0:\n",
    "        for _ in range(num_attention_blocks):\n",
    "            # Normalization and Attention\n",
    "            # \"EMBEDDING LAYER\"\n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "\n",
    "            # \"ATTENTION LAYER\"\n",
    "            x = layers.MultiHeadAttention(\n",
    "                key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "            )(x, x)\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "            res = x + inputs\n",
    "            \n",
    "    if num_attention_blocks == 0:\n",
    "        res = inputs\n",
    "    \n",
    "    if num_transformer_blocks != 0: \n",
    "        for _ in range(num_transformer_blocks):\n",
    "            # FEED FORWARD Part - you can stick anything here or just delete the whole section - it will still work. \n",
    "            x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "            x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation = \"relu\")(x)\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "            x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    \n",
    "    # may need to return something else if transformer blocks are 0\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd22e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_attention_blocks,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0, \n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    \n",
    "    # This is what stacks our transformer blocks\n",
    "    x = transformer_encoder(x, head_size, num_heads, ff_dim, num_attention_blocks, num_transformer_blocks, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"elu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(1, activation=\"linear\")(x) #this is a pass-through\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c387cc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, lr, warmup_epochs=30, decay_epochs=100, initial_lr=1e-6, base_lr=1e-3, min_lr=5e-5):\n",
    "    if epoch <= warmup_epochs:\n",
    "        pct = epoch / warmup_epochs\n",
    "        return ((base_lr - initial_lr) * pct) + initial_lr\n",
    "\n",
    "    if epoch > warmup_epochs and epoch < warmup_epochs+decay_epochs:\n",
    "        pct = 1 - ((epoch - warmup_epochs) / decay_epochs)\n",
    "        return ((base_lr - min_lr) * pct) + min_lr\n",
    "\n",
    "    return min_lr\n",
    "\n",
    "# This learning rate scheduler is from Mr. Theodoros Ntakouris' article at https://towardsdatascience.com/the-time-series-transformer-2a521a0efad3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83058949",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "            keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True),\n",
    "            keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e2e9dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "import pydot\n",
    "import graphviz\n",
    "\n",
    "#tf.keras.utils.plot_model(model, to_file ='model_plot.png', show_shapes = True, show_layer_names = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7d5d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_transformer_predictions_losses(X_train, y_train, X_val, y_val, bar, dir_path, model):\n",
    "    \n",
    "    #Calculate predication for training, validation and test data\n",
    "    print(X_train.shape, X_val.shape)\n",
    "    train_pred = model.predict(X_train)\n",
    "    val_pred = model.predict(X_val)\n",
    "\n",
    "    #Print evaluation metrics for all datasets\n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    val_mse = mean_squared_error(y_val, val_pred)\n",
    "    print(' ')\n",
    "    print('Transformer Evaluation metrics')\n",
    "    print('Transformer Training MSE - MSE: {:.4f}'.format(train_mse)) ### these lines should be MSE???\n",
    "    print('Transformer Validation MSE - MSE: {:.4f}'.format(val_mse))\n",
    "    \n",
    "\n",
    "    plt.plot(y_train, label='Survival')\n",
    "    plt.plot(np.arange(train_pred.shape[0]), train_pred, linewidth=3, label='Predicted Survival')\n",
    "    plt.title(\"Transformer Training Data \"+bar, fontsize=14)\n",
    "    plt.xlabel('Real + Pseudotime')\n",
    "    plt.ylabel('Survival')\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.legend(loc=\"best\", fontsize=12)\n",
    "    plt.savefig(dir_path+bar+'_training.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    #Plot validation data results\n",
    "    plt.plot(y_val, label='Survival')\n",
    "    plt.plot(np.arange(val_pred.shape[0]), val_pred, linewidth=3, label='Predicted Survival')\n",
    "    plt.title(\"Transformer Validation Data \"+bar, fontsize=14)\n",
    "    plt.xlabel('Real + Pseudotime')\n",
    "    plt.ylabel('Survival')\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.legend(loc=\"best\", fontsize=12)\n",
    "    plt.savefig(dir_path+bar+'_validation.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_mse, val_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4af75314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_regr_predictions_losses(X, y, X_test, y_test, bar, dir_path, regr):\n",
    "    \n",
    "    #Calculate predication for training, validation and test data\n",
    "    train_pred = regr.predict(X)\n",
    "    val_pred = regr.predict(X_test)\n",
    "\n",
    "    #Print evaluation metrics for all datasets\n",
    "    train_eval = mean_squared_error(y, train_pred)\n",
    "    val_eval = mean_squared_error(y_test, val_pred)\n",
    "    print(' ')\n",
    "    print('Evaluation metrics Regr')\n",
    "    print('Regr Training Data - MSE: {:.4f}'.format(train_eval)) ### these lines should be MSE???\n",
    "    print('Regr Validation Data - MSE: {:.4f}'.format(val_eval))\n",
    "    \n",
    "    plt.plot(y, label='Survival')\n",
    "    plt.plot(np.arange(train_pred.shape[0]), train_pred, linewidth=3, label='Predicted Survival')\n",
    "    plt.title(\"Regr Training Data \"+bar, fontsize=14)\n",
    "    plt.xlabel('Real + Pseudotime')\n",
    "    plt.ylabel('Survival')\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.legend(loc=\"best\", fontsize=12)\n",
    "    plt.savefig(dir_path+bar+'_training.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    #Plot validation data results\n",
    "    plt.plot(y_test, label='Survival')\n",
    "    plt.plot(np.arange(val_pred.shape[0]), val_pred, linewidth=3, label='Predicted Survival')\n",
    "    plt.title(\"Regr Validation Data \"+bar, fontsize=14)\n",
    "    plt.xlabel('Real + Pseudotime')\n",
    "    plt.ylabel('Survival')\n",
    "    plt.ylim(-1.5, 1.5)\n",
    "    plt.legend(loc=\"best\", fontsize=12)\n",
    "    plt.savefig(dir_path+bar+'_validation.png', bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_eval, val_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a89245a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SHAPvals4(X_val, y_val, bar, COLS, dir_path, model):\n",
    "    \n",
    "    #COLS = bardf.columns.tolist()[1:] # may need fixing\n",
    "    #print(len(COLS), X_val.shape)\n",
    "    #print(COLS)\n",
    "\n",
    "    results = []\n",
    "    print(' Computing feature importance...'+bar)\n",
    "\n",
    "    # COMPUTE BASELINE (NO SHUFFLE)\n",
    "    oof_preds = model.predict(X_val, verbose=0).squeeze() \n",
    "    baseline_mae = np.mean(np.abs( oof_preds-y_val ))\n",
    "    results.append({'feature':'BASELINE','mae':baseline_mae, 'prediction':oof_preds.tolist(), 'true':y_val.tolist()})           \n",
    "\n",
    "    for k in range(len(COLS)):\n",
    "\n",
    "        if k in [20, 40, 60, 80, 100, 120, 140, 160, 180, 200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]:\n",
    "            print('SHAP', k, ' '+bar)\n",
    "        # SHUFFLE FEATURE K\n",
    "        save_col = X_val[:,:,k].copy()\n",
    "        mae_all = []\n",
    "        pred_df = pd.DataFrame() #########\n",
    "        for i in range(30):\n",
    "            np.random.shuffle(X_val[:,:,k])\n",
    "\n",
    "            # COMPUTE OOF MAE WITH FEATURE K SHUFFLED\n",
    "            oof_preds = model.predict(X_val, verbose=0).squeeze() \n",
    "            pred_df['prediction_'+str(i)] = oof_preds #######\n",
    "            mae = np.mean(np.abs( oof_preds-y_val ))\n",
    "            mae_all.append(mae)\n",
    "            X_val[:,:,k] = save_col\n",
    "        results.append({'feature':COLS[k],'mae_mean':np.mean(mae_all), 'mae_std':np.std(mae_all), 'mae_all':mae_all})\n",
    "        for each_col in pred_df.columns.tolist():\n",
    "            results[-1][each_col] = pred_df[each_col].tolist()\n",
    "\n",
    "    # SAVE FEATURE IMPORTANCE\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values('mae_mean',ascending=False)\n",
    "    df['fc_mae'] = df['mae'] - baseline_mae\n",
    "    df.to_csv(dir_path+bar+'_feature_importance_full.csv',index=False)\n",
    "    print('done with '+bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b2548d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seeds produces more similar results, but not exact\n",
    "seed_value = 42\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "num_layers = 10\n",
    "n_heads = 40 # was 40  # Number of attention heads # no difference or imporvement between 5-60\n",
    "n_head_size= 5 # was 46 # Embedding size for attention # no difference between 5-46\n",
    "n_ff_dim = 5  # was 55    # Hidden layer size in feed forward network inside transformer # some difference between 5-55\n",
    "\n",
    "batch_size = 32 # was 32\n",
    "seq_len = 8 # was 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b07db84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember to change the output file names to the appropriate path\n",
    "\n",
    "pfi = True # if you don't want to compute permutation feature importance (lengthy) change to false\n",
    "frac_val = 0.20 # percent of data to be used for testing\n",
    "#test_barcodes = ['GFPBC_libB_92196'] \n",
    "extra_var_bool = [False, False, False] # include 0-realtime, 1-barcode, 2-pseudotime as features for training\n",
    "intervals = [0.25, 0.5, 0.75, 1]\n",
    "pct_test = frac_val/len(intervals) # comment out and use frac_val for last 20%\n",
    "#intervals = [1] # if you only want the final test section as validation\n",
    "#pct_test = frac_val/4 # if you only want the final test section as validation\n",
    "\n",
    "mse_dict = {} # structure is {barcode: {day_0: [trans_train, trans_val, regr_train, regr_val], day_n...}, barcode_n: ...}\n",
    "\n",
    "    \n",
    "def runDynamicDeepLearn(bar):\n",
    "    \n",
    "    print('\\n\\n')\n",
    "    print(bar)\n",
    "    mse_dict[bar] = {}\n",
    "    \n",
    "    for v in intervals:\n",
    "        # make the training and validation data\n",
    "        X_train, y_train, X_val, y_val, COLS = transformer_train_test_split_dayiter(bar, pct_test, extra_var_bool, v)\n",
    "\n",
    "        # build and compile full model\n",
    "        ### the main chunk ###\n",
    "        input_shape = X_train.shape[1:]\n",
    "        model = build_model(\n",
    "            input_shape,\n",
    "            head_size=n_head_size, # Embedding size for attention\n",
    "            num_heads=n_heads, # Number of attention heads\n",
    "            ff_dim=n_ff_dim, # Hidden layer size in feed forward network inside transformer\n",
    "            num_attention_blocks=num_layers, # 3 iterations for attention\n",
    "            num_transformer_blocks=num_layers, # six layers doesn't add or subtract much so best to keep simple\n",
    "            mlp_units=[256],\n",
    "            mlp_dropout=0.4, # unclear if increasing or decreasing has sizable effect\n",
    "            dropout=0.14 # unclear if increasing or decreasing has sizable effect\n",
    "        )\n",
    "        model.compile(\n",
    "            loss=\"mean_squared_error\",\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=1e-3),  # was 1e-4\n",
    "            metrics=[\"mean_squared_error\"],\n",
    "        )\n",
    "        # fit the model\n",
    "        history = model.fit(X_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=20,\n",
    "                        validation_data=(X_val, y_val)) \n",
    "        \n",
    "        # build no attention model\n",
    "        noattn_model = build_model(\n",
    "            input_shape,\n",
    "            head_size=n_head_size, # Embedding size for attention\n",
    "            num_heads=n_heads, # Number of attention heads\n",
    "            ff_dim=n_ff_dim, # Hidden layer size in feed forward network inside transformer\n",
    "            num_attention_blocks=0, # number of iteration layers for attention\n",
    "            num_transformer_blocks=num_layers, # six layers doesn't add or subtract much so best to keep simple\n",
    "            mlp_units=[256],\n",
    "            mlp_dropout=0.4, # unclear if increasing or decreasing has sizable effect\n",
    "            dropout=0.14 # unclear if increasing or decreasing has sizable effect\n",
    "        )\n",
    "        noattn_model.compile(\n",
    "            loss=\"mean_squared_error\",\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=1e-3),  # was 1e-4\n",
    "            metrics=[\"mean_squared_error\"],\n",
    "        )\n",
    "        # fit the model\n",
    "        noattn_history = noattn_model.fit(X_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=20,\n",
    "                        validation_data=(X_val, y_val))\n",
    "        \n",
    "        # build no encode model\n",
    "        noenc_model = build_model(\n",
    "            input_shape,\n",
    "            head_size=n_head_size, # Embedding size for attention\n",
    "            num_heads=n_heads, # Number of attention heads\n",
    "            ff_dim=n_ff_dim, # Hidden layer size in feed forward network inside transformer\n",
    "            num_attention_blocks=num_layers, # number of iteration layers for attention\n",
    "            num_transformer_blocks=0, # six layers doesn't add or subtract much so best to keep simple\n",
    "            mlp_units=[256],\n",
    "            mlp_dropout=0.4, # unclear if increasing or decreasing has sizable effect\n",
    "            dropout=0.14 # unclear if increasing or decreasing has sizable effect\n",
    "        )\n",
    "        noenc_model.compile(\n",
    "            loss=\"mean_squared_error\",\n",
    "            optimizer=keras.optimizers.Adam(learning_rate=1e-3),  # was 1e-4\n",
    "            metrics=[\"mean_squared_error\"],\n",
    "        )\n",
    "        # fit the model\n",
    "        noenc_history = noenc_model.fit(X_train, y_train, \n",
    "                        batch_size=batch_size, \n",
    "                        epochs=20,\n",
    "                        validation_data=(X_val, y_val))\n",
    "\n",
    "\n",
    "        if X_val.shape[0] > 0:\n",
    "\n",
    "            # predictions and loss\n",
    "            train_mse, val_mse = plot_transformer_predictions_losses(X_train, y_train, X_val, y_val, bar, 'test/transformer_ptime_', model)\n",
    "\n",
    "            # noattn - predictions and loss\n",
    "            noattn_train_mse, noattn_val_mse = plot_transformer_predictions_losses(X_train, y_train, X_val, y_val, bar, 'test/noattn_ptime_', noattn_model)\n",
    "            \n",
    "            # noenc - predictions and loss\n",
    "            noenc_train_mse, noenc_val_mse = plot_transformer_predictions_losses(X_train, y_train, X_val, y_val, bar, 'test/noenc_ptime_', noenc_model)\n",
    "            \n",
    "            # build multivariate regression model\n",
    "            X, y, X_test, y_test = regr_train_test_split_dayiter(bar, pct_test, extra_var_bool, v)\n",
    "            regr = linear_model.LinearRegression()\n",
    "            regr.fit(X, y)\n",
    "            regr_train_mse, regr_val_mse = plot_regr_predictions_losses(X, y, X_test, y_test, bar, 'test/regr_ptime_', regr)\n",
    "\n",
    "            # add to mse dictionary\n",
    "            mse_dict[bar][v] = [train_mse, val_mse, regr_train_mse, regr_val_mse, noattn_train_mse, noattn_val_mse, noenc_train_mse, noenc_val_mse]\n",
    "\n",
    "            # SHAP values\n",
    "            if v == 1 and pfi == True:\n",
    "                X_full, y_full, COLS = transformer_train_test_split_dayiter(bar, 0, extra_var_bool, 1)\n",
    "                SHAPvals4(X_full, y_full, bar, COLS, 'test/transformer_full', model)\n",
    "            sys.stdout.flush()\n",
    "                \n",
    "\n",
    "    return mse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2929565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running these barcodes:  ['GFPBC_libB_92196', 'GFPBC_libB_20182', 'GFPBC_libB_90850', 'GFPBC_libB_10678', 'GFPBC_libB_37798', 'GFPBC_libB_31581', 'GFPBC_libB_40758', 'GFPBC_libB_94397', 'GFPBC_libB_83026', 'GFPBC_libB_10737']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GFPBC_libB_37798\n",
      "\n",
      "\n",
      "\n",
      "GFPBC_libB_20182\n",
      "GFPBC_libB_90850GFPBC_libB_10678GFPBC_libB_92196GFPBC_libB_83026GFPBC_libB_40758\n",
      "\n",
      "\n",
      "GFPBC_libB_10737\n",
      "GFPBC_libB_94397\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "GFPBC_libB_31581\n",
      "Transformer Training/Val set shapes (varies depending on barcode) Transformer Training/Val set shapes (varies depending on barcode)   (53, 8, 72)Transformer Training/Val set shapes (varies depending on barcode) (92, 8, 72)  (92, 8, 72)(17, 8, 72) \n",
      "(17, 8, 72)\n",
      "Transformer Training/Val set shapes (varies depending on barcode)   (130, 8, 72)(8, 8, 72) Transformer Training/Val set shapes (varies depending on barcode) \n",
      " (27, 8, 72)(229, 8, 72)\n",
      " Transformer Training/Val set shapes (varies depending on barcode) Transformer Training/Val set shapes (varies depending on barcode) (52, 8, 72)  \n",
      "(110, 8, 72)(395, 8, 72)  (93, 8, 72)\n",
      "(22, 8, 72)\n",
      "Transformer Training/Val set shapes (varies depending on barcode) Transformer Training/Val set shapes (varies depending on barcode)   (432, 8, 72)(469, 8, 72) (111, 8, 72)\n",
      " (102, 8, 72)\n",
      "Transformer Training/Val set shapes (varies depending on barcode)  (690, 8, 72) (167, 8, 72)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-16 11:21:53.838939: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:53.908965: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:53.908965: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:53.917133: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:53.958998: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:53.959006: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:53.962843: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:53.974177: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:53.985074: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2024-01-16 11:21:54.195505: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import sys\n",
    "from multiprocessing import Pool\n",
    "threads = 14\n",
    "print('running these barcodes: ', new_barcodes)\n",
    "\n",
    "    \n",
    "pool = Pool(threads)\n",
    "result = pool.starmap(runDynamicDeepLearn, zip(new_barcodes)) \n",
    "print(result)\n",
    "\n",
    "\n",
    "print(mse_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cbbce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_dict = {}\n",
    "for i, each in enumerate(result):\n",
    "    mse_dict[new_barcodes[i]] = each[new_barcodes[i]]\n",
    "\n",
    "mse = pd.DataFrame(mse_dict)\n",
    "mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb8ba5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the mse data\n",
    "f = open('mse/test_mse.csv', 'w')\n",
    "f.write(str(mse_dict))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ac3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at it\n",
    "mse_file = open('mse/test_mse.csv', 'r')\n",
    "for line in mse_file:\n",
    "    mse = eval(line)\n",
    "mse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (3.10.8) Isolated conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
